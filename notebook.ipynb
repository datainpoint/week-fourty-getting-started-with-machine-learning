{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed4683d",
   "metadata": {},
   "source": [
    "# 約維安計畫：機器學習基礎 \n",
    "\n",
    "> 第四十週\n",
    "\n",
    "![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZmJzZmd1MzI5ZGFydG1nbDJjNGZhbjh6ZmIxczF1NWM4Nnh5NHJ6MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/4TtTVTmBoXp8txRU0C/giphy.gif)\n",
    "\n",
    "## 機器學習演算法\n",
    "\n",
    "截至目前為止，我覺得還是沒有其他對機器學習演算法的定義比得上 Tom Mitchel 的版本，更簡短的版本解釋能力不足、篇幅更長的版本不夠精練，在解釋能力與精練程度上，這段話現階段居於各種版本定義之首，Well…it is definitely “In My Humble Opinion.”\n",
    "\n",
    "> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\n",
    ">\n",
    "> Tom Mitchell\n",
    "\n",
    "一段具備預測數值、預測類別或挖掘特徵的電腦程式（也就是我們日常俗稱的模型），應該符合「三個要素」、「一個但書」的特性。其中，三個要素依序為經驗（Experience）、任務（Task）與效能評估（Performance measure）；一個但書（Condition）則為隨著歷史資料觀測值數量增加，在其他條件不變前提下模型的表現應該要變得更優秀，也就是預測的誤差降低、挖掘資料特徵的能力提升。\n",
    "\n",
    "對於 Tom Mitchel 有興趣的讀者，可以觀看他的 Wikipedia 頁面：<https://en.wikipedia.org/wiki/Tom_M._Mitchell>，或者他在卡內基美隆大學（Carnegie Mellon University, CMU）的頁面：<https://www.cs.cmu.edu/~tom>。\n",
    "\n",
    "## 任務\n",
    "\n",
    "機器學習演算法的需求、動機源自於軟體工程師在撰寫程式解決現實世界的問題時，碰到了「固定程式」難以解決的問題，因此尋求能夠讓電腦程式基於如人工智慧一般的能力來面對這類型問題，常見的機器學習任務有多種：\n",
    "\n",
    "- 分類（Classification）：分類任務會要求電腦程式產生一個函數，這個函數能夠將輸入的資料歸屬成為 k 類別中的其中一個，輸出的資料為離散的形式，例如物件辨識、貸款申請核准與否或疾病診斷陰性陽性等。\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^n \\rightarrow \\{ 1, ..., k \\} \n",
    "$$\n",
    "\n",
    "- 迴歸（Regression）：迴歸任務會要求電腦程式產生一個函數，這個函數能夠將輸入的資料預測某個數值，輸出的資料為連續的形式，例如貸款核准金額、金融商品價格預測或房屋成交價格預測等。\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "- 轉錄（Transcription）：轉錄任務會要求電腦程式產生一個函數，這個函數能夠將輸入的非結構化特徵（Unstructured features）轉換為離散的文字格式，例如光學字元辨識（Optical Character Recognition, OCR）或語音辨識。\n",
    "- 機器翻譯（Machine translation）：機器翻譯任務會要求電腦程式產生一個函數，這個函數能夠將輸入語言的文字符號序列轉換成另一種語言的文字符號序列。\n",
    "- 結構化輸出（Structured output）：結構化輸出任務會要求電腦程式產生一個函數，這個函數能夠將輸入的非結構化特徵轉換為有效、能夠被自然語言理解的文字格式，例如影像標註（Image captioning）、詞性標註。\n",
    "- 異常偵測（Anomaly detection）：異常偵測任務會要求電腦程式產生一個函數，這個函數能夠將輸入一組事件序列輸出為異常標記，例如信用卡詐欺偵測。\n",
    "- 合成與抽樣（Synthesis and sampling）：合成與抽樣任務會要求電腦程式產生一個函數，這個函數能夠將輸入的資料輸出特定類型的格式，例如語音合成能夠從提供的文字輸出一段與該文字相符的語音，目前我們在 Medium 看到每一篇英文文章都有語音合成的功能，將文章轉換為有聲書形式。\n",
    "- 遺漏值填補（Imputation of missing values）：遺漏值填補任務會要求電腦程式產生一個函數，這個函數能夠將輸入的遺漏值資料輸出其預測值。\n",
    "- 去雜訊（Denoising）：去雜訊任務會要求電腦程式產生一個函數，這個函數能夠將輸入的混雜樣本輸出為預測純淨樣本，或更廣泛地輸出預測條件機率分佈。\n",
    "\n",
    "$$\n",
    "f: \\tilde{x} \\rightarrow x \\\\ \n",
    "f: \\tilde{x} \\rightarrow p( x | \\tilde{x} )\n",
    "$$\n",
    "\n",
    "- 密度估計（Density estimation）或機率質量函數估計（Probability mass function estimation）：密度估計任務會要求電腦程式產生一個函數，這個函數能夠將輸入的連續資料輸出為機率密度函數，或者將輸入的離散資料輸出為機率質量函數。\n",
    "\n",
    "$$\n",
    "p : \\mathbb{R}^n \\rightarrow \\mathbb{R} \n",
    "$$\n",
    "\n",
    "## 效能評估\n",
    "\n",
    "當電腦程式依照程式設計者（包含軟體工程師、資料科學家或資料分析師等）的要求順利產生了一個函數，下一步自然要針對這個函數是否能達成任務、能達成多少制定衡量的尺度。對於分類、轉錄等任務而言，常用來做效能評估的是準確率（Accuracy）或者誤差率（Error rate），兩者都是以介於 0 與 1 之間的比率計算，因此又稱為預期的 0-1 損失（Expected 0-1 loss），本質上準確率與誤差率是相同的概念，前者是正面表列而後者是負面表列，如此而已。\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct prediction}}{\\text{Number of total sample}} \\\\ \n",
    "\\text{Error rate} = 1 - \\text{Accuracy}\n",
    "$$\n",
    "\n",
    "對於迴歸、密度估計等任務而言，常用來做效能評估的則是平均平方誤差（Mean squared Error, MSE）或者平均對數機率。\n",
    "\n",
    "機器學習演算法的效能評估著重在「先前沒有遇過」的資料，因此作為效能評估時所使用的測試資料集（Test data）跟訓練機器學習演算法的訓練資料集（Train data）是分開的。效能評估的概念看似簡單、直觀，但是在評估的過程常會遭遇到許多需要抉擇的場景，例如異常偵測任務面對的輸出分佈有極大的落差，是否能夠使用與輸出分佈相對均勻的分類任務同樣的效能評估度量方式？\n",
    "\n",
    "## 經驗\n",
    "\n",
    "機器學習演算法大致上分為非監督式（Unsupervised）或監督式（Supervised），兩者的差異就在於訓練過程中所使用的訓練資料集類型之不同，資料集（Dataset）是許多樣本的集合，而樣本也稱為資料點（Data points）。\n",
    "\n",
    "年代最久遠的資料集之一是家喻戶曉的鳶尾花（Iris dataset），由 150 株鳶尾花不同部位的數值測量集合而成作為特徵，包含三種不同的物種：Setosa、Versicolor 與 Virginica 集合而成作為目標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936de4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.data)\n",
    "print(iris.target)\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98012e0",
   "metadata": {},
   "source": [
    "描述資料及的常見方法是使用設計矩陣（Design matrix），每列對應不同特徵、每欄含有不同樣本的矩陣，例如鳶尾花資料集包含 150 個樣本，每個樣本有四個特徵，因此我們就可以描述一個設計矩陣來表示該資料集。\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{150 \\times 4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3c7a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703eb4",
   "metadata": {},
   "source": [
    "然而若要將資料集描述為設計矩陣，必須將每個樣本都描述成相同長度的向量，這在實務上並非一件容易的事情，例如資料集是不同長、寬、解析度的影像，就不能以相同長度的向量描述，必須要改描述為含有 $m$ 個元素的集合。\n",
    "\n",
    "$$\n",
    "X: \\{ x^{(1)}, x^{(2)}, ..., x^{(m)} \\}\n",
    "$$\n",
    "\n",
    "非監督式學習演算法涵蓋資料集的特徵部分，而監督式學習則涵蓋資料集的特徵與目標部分。具體來說，非監督式學習演算法試圖在觀測隨機向量 $x$ 之後，學習其中的機率分佈 $p(x)$。\n",
    "\n",
    "$$\n",
    "p(x) = \\prod_{i=1}^{n}p(x_i | x_1, ..., x_{i-1})\n",
    "$$\n",
    "\n",
    "而監督式學習演算法則試圖在觀測隨機向量 $x$ 以及其對應向量 $y$ 之後，學習透過估計 $p(y | x)$ 從而達成由 $x$ 預測 $y$。\n",
    "\n",
    "$$\n",
    "p(y | x) = \\frac{p(x, y)}{\\sum_{y^{\\prime}}p(x, y^{\\prime})}\n",
    "$$\n",
    "\n",
    "傳統上，機器學習領域的人們將迴歸、分類與結構化輸出歸納為監督式學習；將密度估計或機率質量函數估計歸納為非監督式學習。此外，也有其他變異型的學習演算法，例如在半監督式學習（Semi-supervised learning）某些樣本涵蓋目標、其他樣本則沒有涵蓋。\n",
    "\n",
    "第四十週約維安計畫：機器學習基礎來到尾聲，希望您也和我一樣期待下一篇文章。對於這篇文章有什麼想法呢？喜歡😻、留言🙋‍♂️、訂閱📨或者分享🙌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
